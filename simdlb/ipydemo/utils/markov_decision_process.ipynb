{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demystify Markov Decision Process\n",
    "\n",
    "(This document is summarized from https://towardsdatascience.com/understanding-the-markov-decision-process-mdp-8f838510f150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Terminology\n",
    "\n",
    "- Agent: an RL agent is the entity which we are training to make correct decisions (e.g., a robot that is being trained to move around)\n",
    "- Environment: where the agens can interact, but it cannot manipulate the environment\n",
    "- State: is to define the current situations of the agent\n",
    "- Action: the choise that an agent makes at the current time step\n",
    "- Policy: is actually a probability distribution assigned to the set of actions that the again can pick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Markov Property\n",
    "\n",
    "- A state $S_{t}$ is Markov if and only if\n",
    "$$\\mathbb{P}[S_{t+1}|S_{t}] = \\mathbb{P}[S_{t+1}|S_{1}, ..., S_{t}]$$\n",
    "\n",
    "- Formally, for a state $S_t$ to be Markov, the probability of the next state $S_{t+1}$ (also can be denoted by $s'$), should only be dependent on the current state $S_t = s_t$, and not on the rest of the past states $S_1 = s_1, S_2 = s_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Markov Process/Chain\n",
    "\n",
    "$$P_{ss'} = \\mathbb{P}[S_{t+1}=s' | S_{t}=s]$$\n",
    "\n",
    "- A Markov process is defined by $(S,P)$ where $S$ is the states, and P is the state-transition probability.\n",
    "- There must be a sequence of random states $S_1, S_2, ...$, where all states must obey the Markov property."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Markov Reward Process\n",
    "\n",
    "- The reward process is defined by $(S, P, R, \\gamma)$, where $S$ is the states, $P$ is the state-transition probability, $R$ is the reqard and $\\gamma$ is the discount factor.\n",
    "\n",
    "- In detail, the state reward $R_s$ is the expected reward over all possible states that one can transition to/from a state $s$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Markov Decision Process\n",
    "\n",
    "- The Markov decision process is defined by $(S, A, P, R, \\gamma)$, where $A$ is the set of actions.\n",
    "- The state transition probability and the state rewards were more or less stochastic (random). However, now the rewards and the next state also depend on what action the agent picks. Basically, the agent can now control its own fate\n",
    "\n",
    "$$ P_{ss'}^{a} =  \\mathbb{P}[S_{t+1} = s' | S_{t} = s, A_{t} = a]$$\n",
    "$$ R_{ss'}^{a} =  \\mathbb{P}[R_{t+1} = s' | S_{t} = s, A_{t} = a]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use case 1: Grid World Problem\n",
    "\n",
    "There is a grid defined by the size of $x$ and $y$ coordinates. Each cell of the grid represents a position of robot or person movement, and each cell has a reward for moving in. Assume we have a start point and an endpoint; the question is how possibility the robot can move to reach the endpoint?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the state and environment for the problem\n",
    "class state(object):    \n",
    "    \"\"\" The class contains information such\n",
    "    + self.x: (int)The x co-ordinate of the state on the grid\n",
    "    + self.y: (int)The y co-ordinate of the state on the grid\n",
    "    + self.loc: (tuple) the actual position of the state on the grid (x,y)\n",
    "    + self.reward: (float) Reward associated with the state\n",
    "    + self.isReachable: (boolean) True if the state is reachable. Unreachable states are blocked by obstacles\n",
    "    + self.isTerminalState: (boolean) True if the the goal location or the fire location\n",
    "    + self.isStartLocation: (boolean) True if at the initial position\n",
    "    + self.freeLocs: (tuple) representing positions not to be blocked such as right and down from the starting\n",
    "    point to allow initial movement\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, x, y, reward=-1, isReachable=True, isTerminalState=False):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.loc = (x,y)\n",
    "        self.reward = reward\n",
    "        self.isReachable = isReachable \n",
    "        self.isTerminalState = isTerminalState\n",
    "        self.isStartLocation = self.isStartLoc()\n",
    "        self.freeLocs = [(0,1), (1,0)]\n",
    "    \n",
    "    # Return true if blockable. Terminal and freeLocs are not blockable\n",
    "    def blockable(self):\n",
    "        return self.playable() and not self.loc in self.freeLocs  \n",
    "    \n",
    "    # Playable states are Reachable non terminal states\n",
    "    def playable(self):\n",
    "        return self.isReachable and not self.isTerminalState\n",
    "    \n",
    "    # Return true if at the starting location\n",
    "    def isStartLoc(self):\n",
    "        if self.loc == (0,0):\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    # To set reward associated with each state\n",
    "    def setReward(self, reward):\n",
    "        self.reward = reward\n",
    "        \n",
    "    # Return true if the state is reachable\n",
    "    def isAccessible(self):\n",
    "        return self.isreachable\n",
    "    \n",
    "    # Add a block to a particular state\n",
    "    def block(self):\n",
    "        self.isReachable = False\n",
    "    \n",
    "    # Return reward for state\n",
    "    def getReward(self):\n",
    "        return self.reward\n",
    "    \n",
    "    # Set a particular state as terminal\n",
    "    def setAsTerminal(self): \n",
    "        self.isTerminalState = True\n",
    "    \n",
    "    # Return True if state is terminal\n",
    "    def isTerminal(self):\n",
    "        return self.isTerminalState"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "# Delare an initial state\n",
    "S = state(1, 1, -1, True, False).blockable()\n",
    "print(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
