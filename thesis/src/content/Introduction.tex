\chapter{Introduction}
\label{ch:Introduction}
\index{Intro!Introduction}
\chaptertoc
\noindent

\section{Overview}
\label{sec:intro_overview}
\index{Intro!Overview}

% -------------------------------------------------------
% Keywords
% -------------------------------------------------------
% - process: a process is the instance of a computer program that is being executed by one or many threads.
% - processors/processor cores: central processing unit (CPU)—also called a central processor or main processor—is the most important processor in a given computer.
% - multi-core processor: is a computer processor integrated circuit with two or more separate processing cores, each of which executes program instructions in parallel.
% - parallelism: is difficult to define because it can appear on different levels.
% - parallel computing: parallel computing refers to the process of breaking down larger problems into smaller, independent, often similar parts that can be executed simultaneously by multiple processors communicating via shared memory.
% - types of parallel computing: bit-level parallelism, ilp, task parallelism.
% - parallel computer: is a machine which has more than one processors.
% - distributed memory machine: is a collection of single computers hooked up with network cables in that each processor can run an independent program and has its own memory without direct access to other processors' memory.
% - supercomputer: supercomputers play an important role in the field of computational science, and are used for a wide range of computationally intensive tasks in various fields. A supercomputer is a computer with a high level of performance as compared to a general-purpose computer. 
% - computer cluster: a computer cluster is a set of computers that work together so that they can be viewed as a single system.
% - HPC: High-performance computing (HPC) uses supercomputers and computer clusters to solve advanced computation problems. High-performance computing (HPC) as a term arose after the term "supercomputing".[3] HPC is sometimes used as a synonym for supercomputing; but, in other contexts, "supercomputer" is used to refer to a more powerful subset of "high-performance computers", and the term "supercomputing" becomes a subset of "high-performance computing".

% -------------------------------------------------------
% From HPC to the objects and instruments of the thesis
% -------------------------------------------------------
\noindent High performance computing (HPC) has gained an important role in solving complex problems and accelerating scientific research. HPC implies using supercomputers to perform computationally intensive operations \cite{sterling2018hpc}. Compared to a general-purpose computer, a supercomputer indicates a machine with high-performance level. The architecture of high-performance computers is designed towards parallel computing that can quickly process such large amounts of data and perform advanced computations.\\
%This is why the architecture refers to parallel computing.\\

% -------------------------------------------------------
% Parallel computing: taxonomy, mem-architectures
% -------------------------------------------------------
Parallel computing is a computational approach in which problems are split into smaller pieces and solved in parallel. Defining parallelism is difficult because it can appear at different levels, e.g., bit-level, instruction-level, data-level, and task-parallelism level \cite{victor2010introhpc}. Over the past two decades, these terms are frequently used to denote parallel computers, which are machines with more than one processor. A processor in computer science is called central processing unit (CPU) that performs operations on a data source. One way to characterize parallel computers is based on the hardware support level, such as multi-core processors \cite{blake2009multicoreprocs} where a processor may consist of multiple cores and each core is a processing element. Another way is Flynn’s characterization \cite{flynn1972comtaxonomy} based on data flow and control flow, which is known as the following four types:
\begin{itemize}
	\item SISD: Single Instruction Single Data
	\item SIMD: Single Instruction Multiple Data
	\item MISD: Multiple Instruction Single Data
	\item MIMD: Multiple Instruction Multiple Data
\end{itemize}
Alongside multiple processors working together, efficient access to memory is a critical consideration. For this reason, we can characterize parallel computers through different types of memory access. The main distinction is between shared memory and distributed memory \cite{jacob2008numavsuma}. Shared memory enables all processors to access the same memory pool, while in distributed memory, each processor has its own physical memory and address space.\\
% Shared memory, exemplified by Uniform Memory Access (UMA) architectures, enables all processors to access the same memory pool. In contrast, the concept of distributed memory, while not a precise synonym for NUMA, is often associated with architectures featuring Non-Uniform Memory Access (NUMA). NUMA introduces a unique dynamic where the access speed to memory can vary for a process depending on whether it is accessing its local memory or that of another processor. This non-uniformity in memory access times is a characteristic feature of NUMA architectures, setting them apart from the uniform memory access experienced in UMA systems.

% -------------------------------------------------------
% Computer clusters and distributed memory machines/systems and parallel apps
% -------------------------------------------------------
Building upon the concept of parallel computers, a supercomputer today refers to a parallel compute cluster, also considered as a common example for distributed memory machines/systems. We define a cluster as a set of parallel computers (called compute nodes) that work together and effectively function as a single system. For example, the SuperMUC-NG supercomputer at the Leibniz Supercomputing Centre (LRZ) of the Bavarian Academy of Sciences and Humanities \cite{lrz2020supermucng} has more than 6400 nodes with 311040 cores and 719 TB memory in total. All nodes are connected to each other via high-speed network, where each node has its private memory, the same hardware, and the same operating system. However, in certain configurations, there may be variations in hardware or operating systems for specific purposes. The scale of clusters may be small as a two-node system or large with many nodes as a supercomputer like SuperMUC-NG. Regardless of the scale, the interconnection among nodes has to be across a network \cite{liem1991mddistributed}. When multiple nodes exchange data, a message-passing technique is needed; and in most cases the open library standard, Message Passing Interface (MPI) \cite{gropp1996mpich}, is used. Commonly, there are different parallel programming models, e.g., shared memory programming models, distributed memory or message passing programming models, data parallel models, and hybrid models. However, it is important to note that almost all applications should be broken into discrete pieces or smaller tasks. Then, the tasks can be executed simultaneously. We consider such applications as task or task-based parallel applications \cite{thoman2018taxonomy}. On one side, executing tasks in parallel should gain speedup in completion time. On the other side, challenges during execution are communication overhead, synchronization, and load balancing.\\

% -------------------------------------------------------
% Thesis' problem and use case: load balancing & iterative apps
% -------------------------------------------------------
\begin{figure}[t]
	\centering
	\includegraphics[scale=0.75]{./pictures/introduction/intro_usecase.pdf}
	\caption{An illustration of iterative execution and load imbalance in distributed memory systems with 4 compute nodes, 2 processes per node.}
	\label{fig:intro_usecase}
\end{figure}

This thesis is concerned with load balancing in distributed memory systems. The goal is to treat each processor with an equal share of the total load \cite{cybenko1989dynamic}. Load balancing is important because a load imbalance might affect the completion time of parallel applications. For example, we assume a given distribution, where each processor is assigned a number of tasks before execution. A processor executing tasks refers to a process that is defined as a logical instance at operating system level. During execution, when all processes are subject to a synchronization barrier but some of them are slower than others, the overall performance will be determined by load imbalance.\\

Regarding the definition of load, load in this thesis refers to the amount of time that a process executes tasks. Occasionally, we might use this amount of time, which a process is active to execute one or more tasks, as the execution time of tasks. Therefore, the values of load and execution time can be understood interchangeably. We target task-based parallel applications with iterative execution. A task is often defined by a compute function, where each task points to a code region and data. ``Iterative'' indicates applications with multiple execution phases that can be repeated over time steps based on user configuration. ``Iterative'' widely refers to bulk synchronous parallel (BSP) models \cite{valiant1990bridging}. Computation in BSP is divided into a sequence of execution phases as shown in Figure \ref{fig:intro_usecase}, which illustrates an iterative execution on 4 nodes, 2 processes per node. The x-axis is the time progress of task execution, where green boxes represent tasks and their length denotes the load value or the execution time. The y-axis lists compute nodes and processes executing tasks in each phase. A phase finishes when all processes are done. At the end of a phase, a barrier is used to ensure synchronization for the next phase.\\

% -------------------------------------------------------
% An overview about approaches in general
% -------------------------------------------------------
In consideration of dealing with imbalance at runtime, load balancing can be classified as ``static'' and ``dynamic'' \cite{xu1996load}.
\begin{itemize}
	\item ``Static'' means using the estimated execution time of tasks per process to balance the load before running applications. Thereby, an accurate cost model for either optimal task assignment or task partitioning algorithms is needed.
	\item ``Dynamic'' means scheduling tasks at runtime without prior knowledge about the load values. Application behavior and system performance variability might change these values at runtime.
\end{itemize}

Specifically, our work are concerned with dynamic load balancing approaches. There are two reference schemes of dynamic load balancing approaches: master-worker \cite{Riakiotakis2011MasterWorkerModel} \cite{Chronopoulos2005ScaleMasterWorkerModel} and work stealing \cite{Blumofe1999OriginWS} \cite{dinan2009scalable}.
\begin{itemize}
	\item Master-worker denotes a scheme, in which the master monitors and distributes the load to all workers. The downside of master-worker is that it is difficult to scale the number of compute nodes up because the master node can be overloaded by an increasing number of worker nodes. Instead of nodes, master process and worker process can be used as alternatives.
	\item Work stealing denotes that we allow an idle process to steal work\footnote{``\textit{Work}'' and ``\textit{task}'' might be used interchangeably.} from the busy ones without prior knowledge. Work stealing can be particularly effective, but its efficiency can be limited by communication overhead in distributed memory systems. Communication overhead can cause delays when processes exchange information and steal tasks.
\end{itemize}

In general, dynamic load balancing depends on a typical context. Our target is to balance the load of a given distribution of tasks over processes running on distributed memory systems. We explore various approaches through the lens of work stealing. The subsequent section outlines our research problem formulation and motivation.

% ``Overloaded'' indicates the load value of a process is larger than average. It also means the process is executing tasks slower than others.

%This drive to improve computation is accomplished by advancing the
%state-of-the-art of computer hardware and software.
%The notion of \gls*{HPC}, the advancement of what is computationally viable
%on computer systems, is also referred to as
%\gls{capability computing}~\cite[1]{NAC2008}.
%\index{HPC|textbf}\index{Capability computing}

% 1.2 Problem Definition and Motivation
\input{content/Introduction-Prob-Form-Motivation}

% 1.3 Research Questions
\input{content/Introduction-Research-Questions}

% 1.4 Methodology and Contribution
\input{content/Introduction-Method-Contribution}

% 1.5 Publication
\input{content/Introduction-Publication}

% 1.6 Outline
\input{content/Introduction-Outline}



